{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e498d7",
   "metadata": {},
   "source": [
    "# Imports & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5632c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import random\n",
    "\n",
    "\n",
    "#Deep Learning - Computer Vision\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "#NLP\n",
    "import spacy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining plotting function\n",
    "def plot_metrics(history, title=None):\n",
    "    fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- F1\n",
    "    ax[2].plot(history.history['f1_score'])\n",
    "    ax[2].plot(history.history['val_f1_score'])\n",
    "    ax[2].set_title('Model F1 Score')\n",
    "    ax[2].set_ylabel('F1 Score')\n",
    "    ax[2].set_xlabel('Epoch')\n",
    "    ax[2].legend(['Train', 'Test'], loc='best')\n",
    "    ax[2].set_ylim((0,1))\n",
    "    ax[2].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[2].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f1f0a",
   "metadata": {},
   "source": [
    "# Loading & preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b661f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "path = \"...\"\n",
    "#cdf = DataFrame with title, brand, description, price etc...\n",
    "cdf_name = \"name.csv\"\n",
    "#idf = images stored as numpy arrays\n",
    "idf_name = 'name.npy'\n",
    "cdf = pd.read_csv(path + tdf_name)\n",
    "Ximg = np.load(path + idf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding the target variable\n",
    "#For the womenswear dataset, there are 6 different styles stored in the ['Style'] column\n",
    "dicat = {'classic':0, 'edgy':1, 'glamour': 2,\n",
    "        'street':3, 'minimalism':4, 'feminity':5}\n",
    "\n",
    "y_raw = cdf['Style'].apply(lambda x: dicat[x])\n",
    "y = to_categorical(y_raw)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a0f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "#Preparing train and test sets\n",
    "X_img_train, X_img_test, y_train, y_test = train_test_split(Ximg, y, test_size=0.25)\n",
    "print(f'X train shape: {X_img_train.shape}')\n",
    "print(f'y train shape: {y_train.shape}')\n",
    "print(f'X test shape: {X_img_test.shape}')\n",
    "print(f'y test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d5660",
   "metadata": {},
   "source": [
    "# ResNet-50 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fba664",
   "metadata": {},
   "source": [
    "First, let's train do transfer learning and train ResNet-50 model to identify the style of clothes from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading resnet\n",
    "def load_model():\n",
    "    model = ResNet50()\n",
    "    return model\n",
    "\n",
    "#freeze the resnet layers in order to only train the layers adapted to our task\n",
    "def set_nontrainable_layers(model):\n",
    "    # Set the first layers to be untrainable\n",
    "    model.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet():\n",
    "    res = load_model()\n",
    "    res = set_nontrainable_layers(res)\n",
    "    resFC = res.get_layer('avg_pool').output\n",
    "    \n",
    "    output = layers.Flatten(name='new_flatten')(resFC)\n",
    "    output = layers.Dense(500, activation='relu', name='dense1')(output)\n",
    "    output = layers.Dense(250, activation='relu', name='dense2')(output)\n",
    "    output = layers.Dense(6, activation='softmax', name='prediction')(output)\n",
    "    resnet_model = Model(res.input, output)\n",
    "\n",
    "    return resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = load_resnet()\n",
    "rmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add parameters to compile the model\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=5000, decay_rate=0.5)\n",
    "adam = Adam(learning_rate=lr_schedule)\n",
    "f1_score = F1Score(num_classes=6)\n",
    "\n",
    "# Compiling model\n",
    "rmodel.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics=['accuracy', 'Precision', f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "rhistory = rmodel.fit(X_img_train, y_train,\n",
    "                  validation_split=0.2, \n",
    "                  epochs = 15, \n",
    "                  verbose = 1,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ecad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel.evaluate(X_img_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d03fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(rhistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b901c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model_path = \"...\"\n",
    "\n",
    "ts = datetime.datetime.now()\n",
    "strs = str(ts)[:10] + \"_\" + str(ts)[11:16]\n",
    "strs = strs.replace(\":\",\"-\")\n",
    "\n",
    "model_file_name = \"rmodel_women_\" + strs\n",
    "rmodel.save(model_path + model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5d17c",
   "metadata": {},
   "source": [
    "# Custom CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273b75f",
   "metadata": {},
   "source": [
    "Now let's try to train a CNN model created for the occasion - and thus specialized in thi specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba22b1",
   "metadata": {},
   "source": [
    "Given that the FC layer of the ResNet model has a dimension of (,2048), let's design the model to have a **similar sized FC layer** in order to have embeddings that are **comparable** for the two models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbc1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    # initialize\n",
    "    model = models.Sequential()\n",
    "\n",
    "    #first layer\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(rate=0.3))\n",
    "\n",
    "    #second layer\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(rate=0.3))\n",
    "    \n",
    "    #third layer\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(rate=0.3))\n",
    "    \n",
    "    #third layer\n",
    "    model.add(layers.Conv2D(24, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.Conv2D(24, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(rate=0.15))\n",
    "    \n",
    "    #flattening before dense\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(150, activation='relu'))\n",
    "\n",
    "    #dense layer\n",
    "    model.add(layers.Dense(75, activation='relu'))\n",
    "\n",
    "    #last classification layer\n",
    "    model.add(layers.Dense(6, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "\n",
    "#Decaying learning rate in the optimizer\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=5000, decay_rate=0.5)\n",
    "adam = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "f1_score = F1Score(num_classes=6)\n",
    "\n",
    "# instantiating model\n",
    "cmodel = initialize_model()\n",
    "cmodel.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics=['accuracy', 'Precision', f1_score], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de472600",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel.summary()\n",
    "#Here we have a (,2400) FC layer - close enough to 2048 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "es = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "chistory = cmodel.fit(X_img_train, y_train,\n",
    "                  validation_split=0.2, \n",
    "                  epochs = 15, \n",
    "                  verbose = 1,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1222f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel.evaluate(X_img_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63666fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(cmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model_path = \"...\"\n",
    "\n",
    "ts = datetime.datetime.now()\n",
    "strs = str(ts)[:10] + \"_\" + str(ts)[11:16]\n",
    "strs = strs.replace(\":\",\"-\")\n",
    "\n",
    "model_file_name = \"cmodel_women\" + strs\n",
    "cmodel.save(model_path + model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0184701",
   "metadata": {},
   "source": [
    "# NLP model (not used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e28d45",
   "metadata": {},
   "source": [
    "Here, I tried to train an NLP model to identify the style of clothes from their description and title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dac082",
   "metadata": {},
   "source": [
    "The model produced good results when trained and used on data sourced from a single website (and thus with description written in a similar vein / with the same style).\n",
    "However, it failed to scale and produce good results when the textual data was diversified and came from different websites (with different description styles, i.e. vocabulary, text length...).\n",
    "As a result, I ended up not using the embeddings extracted from the NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf80ac3",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c31874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating text column with the text of the title and description\n",
    "cdf[\"Text\"] = cdf[\"Title\"] + \" \" + cdf[\"Description\"]\n",
    "cdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch text to lowercase\n",
    "cdf['Text'] = cdf['Text'].apply(lambda x: x.lower())\n",
    "\n",
    "#Handle punctuation\n",
    "def punctualize(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "cdf['Text'] = cdf['Text'].apply(punctualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cec07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep text, no integers\n",
    "def drop_numbers(x):\n",
    "    text = ''.join(word for word in x if not word.isdigit())\n",
    "    return text\n",
    "\n",
    "cdf['Text'] = cdf['Text'].apply(drop_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d8391",
   "metadata": {},
   "source": [
    "To lemmatize the text, I used SpaCy because it produced better results than NLTK/Wordnet, Clip's Pattern and FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords & lemmatization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def stop_lemme_spacy(x):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #stopwords\n",
    "    word_tokens = word_tokenize(x)\n",
    "    text = [w for w in word_tokens if not w in stop_words]\n",
    "    #spaCy lemmatization\n",
    "    doc = nlp(x)\n",
    "    lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_sentence\n",
    "\n",
    "cdf['Text'] = cdf['Text'].apply(stop_lemme_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0520ef",
   "metadata": {},
   "source": [
    "## TF-IDF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used tf-idf to vectorize the text\n",
    "texts = cdf['Text']\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "Xt = tf_idf_vectorizer.fit_transform(texts)\n",
    "\n",
    "#Create DataFrame of vectorized text\n",
    "features_names = tf_idf_vectorizer.get_feature_names()\n",
    "Xt = pd.DataFrame(Xt.toarray(), columns=features_names)\n",
    "Xt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f0d25",
   "metadata": {},
   "source": [
    "To limit the calculating strain on my computer, I below reduce the 'vocabulary' of my vectorized word matrix <br>\n",
    "The goal here is to reduce the dimension of the word matrix while still accounting for more than 90% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa961dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I optimized the size of my vectorized dataframe by reducing its dimension in order to optimize computation\n",
    "\n",
    "num_components = 800 #Reduce the 'vocabulary' of the matrix to 800 vectors\n",
    "svd = TruncatedSVD(n_components=num_components)\n",
    "svd.fit(Xt)\n",
    "print(f'explained total variance ratio: {svd.explained_variance_ratio_.sum()}')\n",
    "axis = np.arange(1, (num_components + 1))\n",
    "plt.plot(axis, svd.explained_variance_ratio_.cumsum());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15de37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new matrix with truncated word vectors\n",
    "latent_df = pd.DataFrame(svd.fit_transform(Xt), index=cdf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f7592",
   "metadata": {},
   "source": [
    "The latent_df matrix contains the text embedding of the description of clothes, which can be used to make recommendations based on the cosine similarity between them (but which I ended up not doing for the reasons given at the beginning of the NLP section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b73962",
   "metadata": {},
   "source": [
    "## Embeddings from a RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef3c2e",
   "metadata": {},
   "source": [
    "Because of the poor quality of the textual data, I had troubles training NLP models and tried different architectures and vectorizing methods. <br>\n",
    "The Deep Learning architecture which produced the best results was use Keras' Tokenizer and a very simple GRU architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(cdf[\"Text\"])\n",
    "for sen in sentences:\n",
    "    X.append(sen)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets choose max length of vectors\n",
    "maxlen = 200\n",
    "\n",
    "### Let's tokenize the vocabulary \n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "print(f'There are {vocab_size} different words in your corpus')\n",
    "#Return tokenized sequences of different length (length : previous text lenght)\n",
    "Xt_train = tk.texts_to_sequences(X_train)\n",
    "Xt_test = tk.texts_to_sequences(X_test)\n",
    "Xt = tk.texts_to_sequences(X)\n",
    "\n",
    "\n",
    "#We need to uniformize the length by padding\n",
    "#Maxlen -> maximum length of train data since it is data the vectorizer is trained on\n",
    "Xp_train = pad_sequences(Xt_train, dtype='float32', padding='post', maxlen=maxlen)\n",
    "Xp_test = pad_sequences(Xt_test, dtype='float32', padding='post', maxlen=maxlen)\n",
    "Xp = pad_sequences(Xt, dtype='float32', padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "print(f'Train shape: {Xp_train.shape}')\n",
    "print(f'Test shape: {Xp_test.shape}')\n",
    "print(f'Whole df shape: {Xp.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnmodel(embedding_size):\n",
    "    #initialization\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    #embedding layer\n",
    "    model.add(layers.Embedding(input_dim=vocab_size + 1, \n",
    "                               output_dim=embedding_size, #size to represent each word\n",
    "                               #input_length=max_sentence_length, #optional\n",
    "                               mask_zero=True))\n",
    "    \n",
    "    #NN layers\n",
    "    model.add(layers.GRU(150, activation='tanh', return_sequences=False))\n",
    "    \n",
    "    model.add(layers.Dense(1000, activation='relu'))\n",
    "        \n",
    "    model.add(layers.Dense(500, activation='relu'))\n",
    "        \n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    \n",
    "    #Output\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "vrnnmodel = rnnmodel(maxlen)\n",
    "vrnnmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=5000, decay_rate=0.5)\n",
    "adam = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "vrnnmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "vrnnhistory = vrnnmodel.fit(Xp_train, y_train,\n",
    "                     validation_split=0.2,\n",
    "                     batch_size=32, \n",
    "                     epochs=15, \n",
    "                     verbose=1,  \n",
    "                     callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6da58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrnnmodel.evaluate(Xp_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b28227",
   "metadata": {},
   "source": [
    "Accuracy = 40%, which is higher than the baseline (1/6 = 16,7%), but not very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"...\"\n",
    "\n",
    "ts = datetime.datetime.now()\n",
    "strs = str(ts)[:10] + \"_\" + str(ts)[11:16]\n",
    "strs = strs.replace(\":\",\"-\")\n",
    "\n",
    "model_file_name = \"NLP_women_\" + strs\n",
    "vrnnmodel.save(model_path + model_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "167px",
    "width": "194px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
